# -*- coding: utf-8 -*-
"""Analyse

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QCw6qXJ7DpyqdSBk2SVsyLDIbhNLj2Ka
"""

!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
air_quality = fetch_ucirepo(id=360)

# data (as pandas dataframes)
X = air_quality.data.features
y = air_quality.data.targets

# metadata
print(air_quality.metadata)

# variable information
print(air_quality.variables)

# ===============================================
# Analyse Complète du Dataset Air Quality UCI
# ===============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from ucimlrepo import fetch_ucirepo

# Configuration pour un meilleur affichage
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.float_format', '{:.2f}'.format)

print("="*70)
print("CHARGEMENT DU DATASET AIR QUALITY")
print("="*70)

# Charger le dataset depuis UCI ML Repository
air_quality = fetch_ucirepo(id=360)

# Extraire les données (features et targets)
X = air_quality.data.features
y = air_quality.data.targets

# Combiner X et y pour l'analyse complète
df = pd.concat([X, y], axis=1)

print("\n✓ Dataset chargé avec succès!\n")

# ===============================================
# 1. MÉTADONNÉES DU DATASET
# ===============================================
print("="*70)
print("MÉTADONNÉES DU DATASET")
print("="*70)
print(air_quality.metadata)
print("\n")

# ===============================================
# 2. INFORMATIONS SUR LES VARIABLES
# ===============================================
print("="*70)
print("INFORMATIONS SUR LES VARIABLES")
print("="*70)
print(air_quality.variables)
print("\n")

# ===============================================
# 3. APERÇU DES DONNÉES
# ===============================================
print("="*70)
print("PREMIÈRES LIGNES DU DATASET")
print("="*70)
print(df.head(10))
print("\n")

print("="*70)
print("DERNIÈRES LIGNES DU DATASET")
print("="*70)
print(df.tail(10))
print("\n")

# ===============================================
# 4. STRUCTURE DU DATASET
# ===============================================
print("="*70)
print("STRUCTURE DU DATASET")
print("="*70)
print(df.info())
print("\n")

print(f"Dimensions du dataset: {df.shape[0]} lignes × {df.shape[1]} colonnes")
print(f"Taille en mémoire: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print("\n")

# ===============================================
# 5. STATISTIQUES DESCRIPTIVES GLOBALES
# ===============================================
print("="*70)
print("STATISTIQUES DESCRIPTIVES GLOBALES")
print("="*70)
print(df.describe())
print("\n")

# ===============================================
# 6. VALEURS MANQUANTES
# ===============================================
print("="*70)
print("ANALYSE DES VALEURS MANQUANTES")
print("="*70)
missing = df.isnull().sum()
missing_pct = (missing / len(df)) * 100
missing_df = pd.DataFrame({
    'Colonne': missing.index,
    'Valeurs Manquantes': missing.values,
    'Pourcentage (%)': missing_pct.values
})
missing_df = missing_df[missing_df['Valeurs Manquantes'] > 0].sort_values(
    'Valeurs Manquantes', ascending=False
)
if len(missing_df) > 0:
    print(missing_df.to_string(index=False))
else:
    print("✓ Aucune valeur manquante détectée!")

# Vérifier les valeurs -200 (convention pour valeurs manquantes)
print("\n" + "="*70)
print("VALEURS -200 (VALEURS MANQUANTES CONVENTIONNELLES)")
print("="*70)
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    count_minus_200 = (df[col] == -200).sum()
    if count_minus_200 > 0:
        pct = (count_minus_200 / len(df)) * 100
        print(f"{col}: {count_minus_200} valeurs (-200) [{pct:.2f}%]")
print("\n")

# ===============================================
# 7. STATISTIQUES DÉTAILLÉES PAR COLONNE
# ===============================================
print("="*70)
print("STATISTIQUES DÉTAILLÉES DES PRINCIPALES VARIABLES")
print("="*70)

# Colonnes numériques importantes (concentrations réelles)
concentration_cols = [col for col in df.columns if 'GT' in col or 'C6H6' in col]

for col in concentration_cols:
    if col in df.columns and df[col].dtype in [np.float64, np.int64]:
        print(f"\n{'─'*70}")
        print(f"Variable: {col}")
        print(f"{'─'*70}")

        # Exclure les valeurs -200 pour les statistiques
        data = df[col][df[col] != -200]

        if len(data) > 0:
            print(f"Nombre de valeurs valides: {len(data)}")
            print(f"Moyenne: {data.mean():.2f}")
            print(f"Médiane: {data.median():.2f}")
            if not data.mode().empty:
                print(f"Mode: {data.mode()[0]:.2f}")
            print(f"Écart-type: {data.std():.2f}")
            print(f"Variance: {data.var():.2f}")
            print(f"Min: {data.min():.2f}")
            print(f"Max: {data.max():.2f}")
            print(f"Étendue: {data.max() - data.min():.2f}")

            # Quartiles
            print(f"\nQuartiles:")
            print(f"  Q1 (25%): {data.quantile(0.25):.2f}")
            print(f"  Q2 (50% - Médiane): {data.quantile(0.50):.2f}")
            print(f"  Q3 (75%): {data.quantile(0.75):.2f}")
            print(f"  IQR (Écart interquartile): {data.quantile(0.75) - data.quantile(0.25):.2f}")

            # Asymétrie et aplatissement
            print(f"\nAsymétrie (Skewness): {data.skew():.2f}")
            print(f"Aplatissement (Kurtosis): {data.kurtosis():.2f}")

print("\n")

# ===============================================
# 8. STATISTIQUES DES CAPTEURS
# ===============================================
print("="*70)
print("STATISTIQUES DES RÉPONSES DES CAPTEURS")
print("="*70)

sensor_cols = [col for col in df.columns if 'PT08' in col]

for col in sensor_cols:
    if col in df.columns and df[col].dtype in [np.float64, np.int64]:
        print(f"\n{'─'*70}")
        print(f"Capteur: {col}")
        print(f"{'─'*70}")

        data = df[col][df[col] != -200]

        if len(data) > 0:
            print(f"Nombre de mesures: {len(data)}")
            print(f"Moyenne: {data.mean():.2f}")
            print(f"Médiane: {data.median():.2f}")
            print(f"Écart-type: {data.std():.2f}")
            print(f"Min: {data.min():.2f}")
            print(f"Max: {data.max():.2f}")
            print(f"Q1: {data.quantile(0.25):.2f} | Q2: {data.quantile(0.50):.2f} | Q3: {data.quantile(0.75):.2f}")

print("\n")

# ===============================================
# 9. VARIABLES ENVIRONNEMENTALES
# ===============================================
print("="*70)
print("STATISTIQUES DES VARIABLES ENVIRONNEMENTALES")
print("="*70)

env_vars = ['T', 'RH', 'AH']  # Température, Humidité Relative, Humidité Absolue

for var in env_vars:
    # Rechercher la colonne correspondante
    matching_cols = [col for col in df.columns if var in col]

    for col in matching_cols:
        if df[col].dtype in [np.float64, np.int64]:
            print(f"\n{'─'*70}")
            print(f"Variable Environnementale: {col}")
            print(f"{'─'*70}")

            data = df[col][df[col] != -200]

            if len(data) > 0:
                print(f"Nombre de mesures: {len(data)}")
                print(f"Moyenne: {data.mean():.2f}")
                print(f"Médiane: {data.median():.2f}")
                print(f"Écart-type: {data.std():.2f}")
                print(f"Min: {data.min():.2f}")
                print(f"Max: {data.max():.2f}")
                print(f"Étendue: {data.max() - data.min():.2f}")

print("\n")

# ===============================================
# 10. CORRÉLATIONS
# ===============================================
print("="*70)
print("MATRICE DE CORRÉLATION (TOP 10 CORRÉLATIONS)")
print("="*70)

# Sélectionner uniquement les colonnes numériques et exclure -200
numeric_df = df.select_dtypes(include=[np.number])
numeric_df_clean = numeric_df.replace(-200, np.nan)

# Calculer la matrice de corrélation
corr_matrix = numeric_df_clean.corr()

# Extraire les corrélations en excluant la diagonale
corr_pairs = corr_matrix.unstack()
corr_pairs = corr_pairs[corr_pairs != 1.0]  # Exclure auto-corrélations
corr_pairs = corr_pairs.drop_duplicates()
corr_pairs = corr_pairs.sort_values(ascending=False)

print("\nTop 10 Corrélations Positives:")
print(corr_pairs.head(10))

print("\nTop 10 Corrélations Négatives:")
print(corr_pairs.tail(10))

print("\n")

# ===============================================
# 11. RÉSUMÉ FINAL
# ===============================================
print("="*70)
print("RÉSUMÉ DE L'ANALYSE")
print("="*70)
print(f"✓ Dataset: Air Quality UCI")
print(f"✓ Période: Mars 2004 - Février 2005")
print(f"✓ Nombre total d'observations: {len(df)}")
print(f"✓ Nombre de variables: {len(df.columns)}")
print(f"✓ Variables numériques: {len(numeric_cols)}")
print(f"✓ Taille du dataset: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print("\n✓ Analyse terminée avec succès!")
print("="*70)

# ===============================================
# Analyse Complète du Dataset Air Quality UCI
# Avec Visualisations Graphiques
# ===============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from ucimlrepo import fetch_ucirepo
import warnings
warnings.filterwarnings('ignore')

# Configuration pour un meilleur affichage
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.float_format', '{:.2f}'.format)

# Configuration des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (15, 8)
plt.rcParams['font.size'] = 10

print("="*70)
print("CHARGEMENT DU DATASET AIR QUALITY")
print("="*70)

# Charger le dataset depuis UCI ML Repository
air_quality = fetch_ucirepo(id=360)

# Extraire les données (features et targets)
X = air_quality.data.features
y = air_quality.data.targets

# Combiner X et y pour l'analyse complète
df = pd.concat([X, y], axis=1)

print("\n✓ Dataset chargé avec succès!\n")

# ===============================================
# 1. MÉTADONNÉES DU DATASET
# ===============================================
print("="*70)
print("MÉTADONNÉES DU DATASET")
print("="*70)
print(air_quality.metadata)
print("\n")

# ===============================================
# 2. INFORMATIONS SUR LES VARIABLES
# ===============================================
print("="*70)
print("INFORMATIONS SUR LES VARIABLES")
print("="*70)
print(air_quality.variables)
print("\n")

# ===============================================
# 3. APERÇU DES DONNÉES
# ===============================================
print("="*70)
print("PREMIÈRES LIGNES DU DATASET")
print("="*70)
print(df.head(10))
print("\n")

print("="*70)
print("DERNIÈRES LIGNES DU DATASET")
print("="*70)
print(df.tail(10))
print("\n")

# ===============================================
# 4. STRUCTURE DU DATASET
# ===============================================
print("="*70)
print("STRUCTURE DU DATASET")
print("="*70)
print(df.info())
print("\n")

print(f"Dimensions du dataset: {df.shape[0]} lignes × {df.shape[1]} colonnes")
print(f"Taille en mémoire: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print("\n")

# ===============================================
# 5. STATISTIQUES DESCRIPTIVES GLOBALES
# ===============================================
print("="*70)
print("STATISTIQUES DESCRIPTIVES GLOBALES")
print("="*70)
print(df.describe())
print("\n")

# ===============================================
# 6. VALEURS MANQUANTES
# ===============================================
print("="*70)
print("ANALYSE DES VALEURS MANQUANTES")
print("="*70)
missing = df.isnull().sum()
missing_pct = (missing / len(df)) * 100
missing_df = pd.DataFrame({
    'Colonne': missing.index,
    'Valeurs Manquantes': missing.values,
    'Pourcentage (%)': missing_pct.values
})
missing_df = missing_df[missing_df['Valeurs Manquantes'] > 0].sort_values(
    'Valeurs Manquantes', ascending=False
)
if len(missing_df) > 0:
    print(missing_df.to_string(index=False))
else:
    print("✓ Aucune valeur manquante détectée!")

# Vérifier les valeurs -200 (convention pour valeurs manquantes)
print("\n" + "="*70)
print("VALEURS -200 (VALEURS MANQUANTES CONVENTIONNELLES)")
print("="*70)
numeric_cols = df.select_dtypes(include=[np.number]).columns
missing_200_data = []
for col in numeric_cols:
    count_minus_200 = (df[col] == -200).sum()
    if count_minus_200 > 0:
        pct = (count_minus_200 / len(df)) * 100
        print(f"{col}: {count_minus_200} valeurs (-200) [{pct:.2f}%]")
        missing_200_data.append({'Colonne': col, 'Count': count_minus_200, 'Percentage': pct})

# GRAPHIQUE 1: Valeurs manquantes (-200)
if missing_200_data:
    fig, ax = plt.subplots(figsize=(12, 6))
    missing_200_df = pd.DataFrame(missing_200_data)
    missing_200_df = missing_200_df.sort_values('Count', ascending=True)

    bars = ax.barh(missing_200_df['Colonne'], missing_200_df['Count'], color='coral')
    ax.set_xlabel('Nombre de valeurs -200', fontsize=12, fontweight='bold')
    ax.set_ylabel('Variables', fontsize=12, fontweight='bold')
    ax.set_title('Distribution des Valeurs Manquantes (-200) par Variable',
                 fontsize=14, fontweight='bold', pad=20)

    # Ajouter les pourcentages sur les barres
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax.text(width, bar.get_y() + bar.get_height()/2,
                f' {missing_200_df.iloc[i]["Percentage"]:.1f}%',
                va='center', fontweight='bold')

    plt.tight_layout()
    plt.show()

print("\n")

# ===============================================
# 7. STATISTIQUES DÉTAILLÉES PAR COLONNE
# ===============================================
print("="*70)
print("STATISTIQUES DÉTAILLÉES DES PRINCIPALES VARIABLES")
print("="*70)

# Colonnes numériques importantes (concentrations réelles)
concentration_cols = [col for col in df.columns if 'GT' in col or 'C6H6' in col]

stats_summary = []
for col in concentration_cols:
    if col in df.columns and df[col].dtype in [np.float64, np.int64]:
        print(f"\n{'─'*70}")
        print(f"Variable: {col}")
        print(f"{'─'*70}")

        # Exclure les valeurs -200 pour les statistiques
        data = df[col][df[col] != -200]

        if len(data) > 0:
            print(f"Nombre de valeurs valides: {len(data)}")
            print(f"Moyenne: {data.mean():.2f}")
            print(f"Médiane: {data.median():.2f}")
            if not data.mode().empty:
                print(f"Mode: {data.mode()[0]:.2f}")
            print(f"Écart-type: {data.std():.2f}")
            print(f"Variance: {data.var():.2f}")
            print(f"Min: {data.min():.2f}")
            print(f"Max: {data.max():.2f}")
            print(f"Étendue: {data.max() - data.min():.2f}")

            # Quartiles
            print(f"\nQuartiles:")
            print(f"  Q1 (25%): {data.quantile(0.25):.2f}")
            print(f"  Q2 (50% - Médiane): {data.quantile(0.50):.2f}")
            print(f"  Q3 (75%): {data.quantile(0.75):.2f}")
            print(f"  IQR (Écart interquartile): {data.quantile(0.75) - data.quantile(0.25):.2f}")

            # Asymétrie et aplatissement
            print(f"\nAsymétrie (Skewness): {data.skew():.2f}")
            print(f"Aplatissement (Kurtosis): {data.kurtosis():.2f}")

            stats_summary.append({
                'Variable': col,
                'Moyenne': data.mean(),
                'Médiane': data.median(),
                'Écart-type': data.std(),
                'Min': data.min(),
                'Max': data.max()
            })

print("\n")

# GRAPHIQUE 2: Distribution des concentrations de polluants
if concentration_cols:
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.ravel()

    for idx, col in enumerate(concentration_cols[:6]):
        if col in df.columns:
            data = df[col][df[col] != -200]
            if len(data) > 0:
                axes[idx].hist(data, bins=50, color='skyblue', edgecolor='black', alpha=0.7)
                axes[idx].axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Moyenne: {data.mean():.2f}')
                axes[idx].axvline(data.median(), color='green', linestyle='--', linewidth=2, label=f'Médiane: {data.median():.2f}')
                axes[idx].set_xlabel('Concentration', fontweight='bold')
                axes[idx].set_ylabel('Fréquence', fontweight='bold')
                axes[idx].set_title(f'Distribution - {col}', fontweight='bold', fontsize=11)
                axes[idx].legend()
                axes[idx].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.suptitle('Distributions des Concentrations de Polluants',
                 fontsize=16, fontweight='bold', y=1.02)
    plt.show()

# GRAPHIQUE 3: Box plots des concentrations
if concentration_cols:
    fig, ax = plt.subplots(figsize=(14, 6))

    data_for_boxplot = []
    labels_for_boxplot = []

    for col in concentration_cols:
        if col in df.columns:
            data = df[col][df[col] != -200]
            if len(data) > 0:
                data_for_boxplot.append(data)
                labels_for_boxplot.append(col)

    bp = ax.boxplot(data_for_boxplot, labels=labels_for_boxplot, patch_artist=True)

    # Colorier les boîtes
    colors = plt.cm.Set3(np.linspace(0, 1, len(bp['boxes'])))
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    ax.set_ylabel('Concentration', fontsize=12, fontweight='bold')
    ax.set_title('Box Plots des Concentrations de Polluants',
                 fontsize=14, fontweight='bold', pad=20)
    ax.grid(True, alpha=0.3)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# ===============================================
# 8. STATISTIQUES DES CAPTEURS
# ===============================================
print("="*70)
print("STATISTIQUES DES RÉPONSES DES CAPTEURS")
print("="*70)

sensor_cols = [col for col in df.columns if 'PT08' in col]

for col in sensor_cols:
    if col in df.columns and df[col].dtype in [np.float64, np.int64]:
        print(f"\n{'─'*70}")
        print(f"Capteur: {col}")
        print(f"{'─'*70}")

        data = df[col][df[col] != -200]

        if len(data) > 0:
            print(f"Nombre de mesures: {len(data)}")
            print(f"Moyenne: {data.mean():.2f}")
            print(f"Médiane: {data.median():.2f}")
            print(f"Écart-type: {data.std():.2f}")
            print(f"Min: {data.min():.2f}")
            print(f"Max: {data.max():.2f}")
            print(f"Q1: {data.quantile(0.25):.2f} | Q2: {data.quantile(0.50):.2f} | Q3: {data.quantile(0.75):.2f}")

print("\n")

# GRAPHIQUE 4: Réponses des capteurs dans le temps
if sensor_cols:
    fig, axes = plt.subplots(len(sensor_cols), 1, figsize=(16, 3*len(sensor_cols)))

    if len(sensor_cols) == 1:
        axes = [axes]

    for idx, col in enumerate(sensor_cols):
        if col in df.columns:
            data = df[col].replace(-200, np.nan)
            axes[idx].plot(data, linewidth=0.5, alpha=0.7)
            axes[idx].set_ylabel('Réponse', fontweight='bold')
            axes[idx].set_title(f'Série Temporelle - {col}', fontweight='bold', fontsize=11)
            axes[idx].grid(True, alpha=0.3)

    axes[-1].set_xlabel('Index temporel', fontweight='bold')
    plt.tight_layout()
    plt.suptitle('Évolution Temporelle des Réponses des Capteurs',
                 fontsize=16, fontweight='bold', y=1.001)
    plt.show()

# ===============================================
# 9. VARIABLES ENVIRONNEMENTALES
# ===============================================
print("="*70)
print("STATISTIQUES DES VARIABLES ENVIRONNEMENTALES")
print("="*70)

env_vars = ['T', 'RH', 'AH']
env_data = {}

for var in env_vars:
    matching_cols = [col for col in df.columns if var in col]

    for col in matching_cols:
        if df[col].dtype in [np.float64, np.int64]:
            print(f"\n{'─'*70}")
            print(f"Variable Environnementale: {col}")
            print(f"{'─'*70}")

            data = df[col][df[col] != -200]

            if len(data) > 0:
                print(f"Nombre de mesures: {len(data)}")
                print(f"Moyenne: {data.mean():.2f}")
                print(f"Médiane: {data.median():.2f}")
                print(f"Écart-type: {data.std():.2f}")
                print(f"Min: {data.min():.2f}")
                print(f"Max: {data.max():.2f}")
                print(f"Étendue: {data.max() - data.min():.2f}")

                env_data[col] = data

print("\n")

# GRAPHIQUE 5: Variables environnementales
if env_data:
    fig, axes = plt.subplots(len(env_data), 1, figsize=(16, 4*len(env_data)))

    if len(env_data) == 1:
        axes = [axes]

    for idx, (col, data) in enumerate(env_data.items()):
        axes[idx].plot(data.index, data.values, color='orange', linewidth=0.8, alpha=0.7)
        axes[idx].fill_between(data.index, data.values, alpha=0.3, color='orange')
        axes[idx].axhline(y=data.mean(), color='red', linestyle='--', label=f'Moyenne: {data.mean():.2f}')
        axes[idx].set_ylabel(col, fontweight='bold')
        axes[idx].set_title(f'Évolution - {col}', fontweight='bold', fontsize=12)
        axes[idx].legend()
        axes[idx].grid(True, alpha=0.3)

    axes[-1].set_xlabel('Index temporel', fontweight='bold')
    plt.tight_layout()
    plt.suptitle('Variables Environnementales dans le Temps',
                 fontsize=16, fontweight='bold', y=1.001)
    plt.show()

# ===============================================
# 10. CORRÉLATIONS
# ===============================================
print("="*70)
print("MATRICE DE CORRÉLATION")
print("="*70)

# Sélectionner uniquement les colonnes numériques et exclure -200
numeric_df = df.select_dtypes(include=[np.number])
numeric_df_clean = numeric_df.replace(-200, np.nan)

# Calculer la matrice de corrélation
corr_matrix = numeric_df_clean.corr()

# Extraire les corrélations en excluant la diagonale
corr_pairs = corr_matrix.unstack()
corr_pairs = corr_pairs[corr_pairs != 1.0]
corr_pairs = corr_pairs.drop_duplicates()
corr_pairs = corr_pairs.sort_values(ascending=False)

print("\nTop 10 Corrélations Positives:")
print(corr_pairs.head(10))

print("\nTop 10 Corrélations Négatives:")
print(corr_pairs.tail(10))

print("\n")

# GRAPHIQUE 6: Heatmap de corrélation
fig, ax = plt.subplots(figsize=(14, 12))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            center=0, square=True, linewidths=0.5, cbar_kws={"shrink": 0.8},
            ax=ax)
ax.set_title('Matrice de Corrélation - Variables du Dataset Air Quality',
             fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

# GRAPHIQUE 7: Top corrélations (barplot)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Top 15 corrélations positives
top_positive = corr_pairs.head(15)
top_positive_labels = [f"{pair[0][:10]} vs {pair[1][:10]}" for pair in top_positive.index]
ax1.barh(range(len(top_positive)), top_positive.values, color='green', alpha=0.7)
ax1.set_yticks(range(len(top_positive)))
ax1.set_yticklabels(top_positive_labels, fontsize=9)
ax1.set_xlabel('Coefficient de Corrélation', fontweight='bold')
ax1.set_title('Top 15 Corrélations Positives', fontweight='bold', fontsize=12)
ax1.grid(True, alpha=0.3, axis='x')

# Top 15 corrélations négatives
top_negative = corr_pairs.tail(15)
top_negative_labels = [f"{pair[0][:10]} vs {pair[1][:10]}" for pair in top_negative.index]
ax2.barh(range(len(top_negative)), top_negative.values, color='red', alpha=0.7)
ax2.set_yticks(range(len(top_negative)))
ax2.set_yticklabels(top_negative_labels, fontsize=9)
ax2.set_xlabel('Coefficient de Corrélation', fontweight='bold')
ax2.set_title('Top 15 Corrélations Négatives', fontweight='bold', fontsize=12)
ax2.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# GRAPHIQUE 8: Scatter plots des corrélations importantes
if len(concentration_cols) >= 2:
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    axes = axes.ravel()

    # Sélectionner quelques paires de variables intéressantes
    plot_count = 0
    for i in range(len(concentration_cols)):
        for j in range(i+1, len(concentration_cols)):
            if plot_count < 4:
                col1, col2 = concentration_cols[i], concentration_cols[j]
                if col1 in df.columns and col2 in df.columns:
                    data1 = df[col1].replace(-200, np.nan)
                    data2 = df[col2].replace(-200, np.nan)

                    axes[plot_count].scatter(data1, data2, alpha=0.3, s=10)
                    axes[plot_count].set_xlabel(col1, fontweight='bold')
                    axes[plot_count].set_ylabel(col2, fontweight='bold')

                    # Calculer la corrélation
                    valid_mask = ~(data1.isna() | data2.isna())
                    if valid_mask.sum() > 0:
                        corr_val = data1[valid_mask].corr(data2[valid_mask])
                        axes[plot_count].set_title(f'{col1} vs {col2}\nCorr: {corr_val:.3f}',
                                                   fontweight='bold', fontsize=10)

                    axes[plot_count].grid(True, alpha=0.3)
                    plot_count += 1

    plt.tight_layout()
    plt.suptitle('Relations entre Concentrations de Polluants',
                 fontsize=14, fontweight='bold', y=1.001)
    plt.show()

# ===============================================
# 11. RÉSUMÉ FINAL
# ===============================================
print("="*70)
print("RÉSUMÉ DE L'ANALYSE")
print("="*70)
print(f"✓ Dataset: Air Quality UCI")
print(f"✓ Période: Mars 2004 - Février 2005")
print(f"✓ Nombre total d'observations: {len(df)}")
print(f"✓ Nombre de variables: {len(df.columns)}")
print(f"✓ Variables numériques: {len(numeric_cols)}")
print(f"✓ Taille du dataset: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print(f"\n✓ Graphiques générés: 8")
print("  1. Distribution des valeurs manquantes")
print("  2. Histogrammes des concentrations de polluants")
print("  3. Box plots des concentrations")
print("  4. Séries temporelles des capteurs")
print("  5. Variables environnementales")
print("  6. Heatmap de corrélation")
print("  7. Top corrélations (positives/négatives)")
print("  8. Scatter plots des relations entre polluants")
print("\n✓ Analyse terminée avec succès!")
print("="*70)