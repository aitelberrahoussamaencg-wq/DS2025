# -*- coding: utf-8 -*-
"""Analyse.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QCw6qXJ7DpyqdSBk2SVsyLDIbhNLj2Ka
"""

!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
air_quality = fetch_ucirepo(id=360)

# data (as pandas dataframes)
X = air_quality.data.features
y = air_quality.data.targets

# metadata
print(air_quality.metadata)

# variable information
print(air_quality.variables)

# ===============================================
# Analyse Complète du Dataset Air Quality UCI
# ===============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from ucimlrepo import fetch_ucirepo

# Configuration pour un meilleur affichage
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.float_format', '{:.2f}'.format)

print("="*70)
print("CHARGEMENT DU DATASET AIR QUALITY")
print("="*70)

# Charger le dataset depuis UCI ML Repository
air_quality = fetch_ucirepo(id=360)

# Extraire les données (features et targets)
X = air_quality.data.features
y = air_quality.data.targets

# Combiner X et y pour l'analyse complète
df = pd.concat([X, y], axis=1)

print("\n✓ Dataset chargé avec succès!\n")

# ===============================================
# 1. MÉTADONNÉES DU DATASET
# ===============================================
print("="*70)
print("MÉTADONNÉES DU DATASET")
print("="*70)
print(air_quality.metadata)
print("\n")

# ===============================================
# 2. INFORMATIONS SUR LES VARIABLES
# ===============================================
print("="*70)
print("INFORMATIONS SUR LES VARIABLES")
print("="*70)
print(air_quality.variables)
print("\n")

# ===============================================
# 3. APERÇU DES DONNÉES
# ===============================================
print("="*70)
print("PREMIÈRES LIGNES DU DATASET")
print("="*70)
print(df.head(10))
print("\n")

print("="*70)
print("DERNIÈRES LIGNES DU DATASET")
print("="*70)
print(df.tail(10))
print("\n")

# ===============================================
# 4. STRUCTURE DU DATASET
# ===============================================
print("="*70)
print("STRUCTURE DU DATASET")
print("="*70)
print(df.info())
print("\n")

print(f"Dimensions du dataset: {df.shape[0]} lignes × {df.shape[1]} colonnes")
print(f"Taille en mémoire: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print("\n")

# ===============================================
# 5. STATISTIQUES DESCRIPTIVES GLOBALES
# ===============================================
print("="*70)
print("STATISTIQUES DESCRIPTIVES GLOBALES")
print("="*70)
print(df.describe())
print("\n")

# ===============================================
# 6. VALEURS MANQUANTES
# ===============================================
print("="*70)
print("ANALYSE DES VALEURS MANQUANTES")
print("="*70)
missing = df.isnull().sum()
missing_pct = (missing / len(df)) * 100
missing_df = pd.DataFrame({
    'Colonne': missing.index,
    'Valeurs Manquantes': missing.values,
    'Pourcentage (%)': missing_pct.values
})
missing_df = missing_df[missing_df['Valeurs Manquantes'] > 0].sort_values(
    'Valeurs Manquantes', ascending=False
)
if len(missing_df) > 0:
    print(missing_df.to_string(index=False))
else:
    print("✓ Aucune valeur manquante détectée!")

# Vérifier les valeurs -200 (convention pour valeurs manquantes)
print("\n" + "="*70)
print("VALEURS -200 (VALEURS MANQUANTES CONVENTIONNELLES)")
print("="*70)
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    count_minus_200 = (df[col] == -200).sum()
    if count_minus_200 > 0:
        pct = (count_minus_200 / len(df)) * 100
        print(f"{col}: {count_minus_200} valeurs (-200) [{pct:.2f}%]")
print("\n")

# ===============================================
# 7. STATISTIQUES DÉTAILLÉES PAR COLONNE
# ===============================================
print("="*70)
print("STATISTIQUES DÉTAILLÉES DES PRINCIPALES VARIABLES")
print("="*70)

# Colonnes numériques importantes (concentrations réelles)
concentration_cols = [col for col in df.columns if 'GT' in col or 'C6H6' in col]

for col in concentration_cols:
    if col in df.columns and df[col].dtype in [np.float64, np.int64]:
        print(f"\n{'─'*70}")
        print(f"Variable: {col}")
        print(f"{'─'*70}")

        # Exclure les valeurs -200 pour les statistiques
        data = df[col][df[col] != -200]

        if len(data) > 0:
            print(f"Nombre de valeurs valides: {len(data)}")
            print(f"Moyenne: {data.mean():.2f}")
            print(f"Médiane: {data.median():.2f}")
            if not data.mode().empty:
                print(f"Mode: {data.mode()[0]:.2f}")
            print(f"Écart-type: {data.std():.2f}")
            print(f"Variance: {data.var():.2f}")
            print(f"Min: {data.min():.2f}")
            print(f"Max: {data.max():.2f}")
            print(f"Étendue: {data.max() - data.min():.2f}")

            # Quartiles
            print(f"\nQuartiles:")
            print(f"  Q1 (25%): {data.quantile(0.25):.2f}")
            print(f"  Q2 (50% - Médiane): {data.quantile(0.50):.2f}")
            print(f"  Q3 (75%): {data.quantile(0.75):.2f}")
            print(f"  IQR (Écart interquartile): {data.quantile(0.75) - data.quantile(0.25):.2f}")

            # Asymétrie et aplatissement
            print(f"\nAsymétrie (Skewness): {data.skew():.2f}")
            print(f"Aplatissement (Kurtosis): {data.kurtosis():.2f}")

print("\n")

# ===============================================
# 8. STATISTIQUES DES CAPTEURS
# ===============================================
print("="*70)
print("STATISTIQUES DES RÉPONSES DES CAPTEURS")
print("="*70)

sensor_cols = [col for col in df.columns if 'PT08' in col]

for col in sensor_cols:
    if col in df.columns and df[col].dtype in [np.float64, np.int64]:
        print(f"\n{'─'*70}")
        print(f"Capteur: {col}")
        print(f"{'─'*70}")

        data = df[col][df[col] != -200]

        if len(data) > 0:
            print(f"Nombre de mesures: {len(data)}")
            print(f"Moyenne: {data.mean():.2f}")
            print(f"Médiane: {data.median():.2f}")
            print(f"Écart-type: {data.std():.2f}")
            print(f"Min: {data.min():.2f}")
            print(f"Max: {data.max():.2f}")
            print(f"Q1: {data.quantile(0.25):.2f} | Q2: {data.quantile(0.50):.2f} | Q3: {data.quantile(0.75):.2f}")

print("\n")

# ===============================================
# 9. VARIABLES ENVIRONNEMENTALES
# ===============================================
print("="*70)
print("STATISTIQUES DES VARIABLES ENVIRONNEMENTALES")
print("="*70)

env_vars = ['T', 'RH', 'AH']  # Température, Humidité Relative, Humidité Absolue

for var in env_vars:
    # Rechercher la colonne correspondante
    matching_cols = [col for col in df.columns if var in col]

    for col in matching_cols:
        if df[col].dtype in [np.float64, np.int64]:
            print(f"\n{'─'*70}")
            print(f"Variable Environnementale: {col}")
            print(f"{'─'*70}")

            data = df[col][df[col] != -200]

            if len(data) > 0:
                print(f"Nombre de mesures: {len(data)}")
                print(f"Moyenne: {data.mean():.2f}")
                print(f"Médiane: {data.median():.2f}")
                print(f"Écart-type: {data.std():.2f}")
                print(f"Min: {data.min():.2f}")
                print(f"Max: {data.max():.2f}")
                print(f"Étendue: {data.max() - data.min():.2f}")

print("\n")

# ===============================================
# 10. CORRÉLATIONS
# ===============================================
print("="*70)
print("MATRICE DE CORRÉLATION (TOP 10 CORRÉLATIONS)")
print("="*70)

# Sélectionner uniquement les colonnes numériques et exclure -200
numeric_df = df.select_dtypes(include=[np.number])
numeric_df_clean = numeric_df.replace(-200, np.nan)

# Calculer la matrice de corrélation
corr_matrix = numeric_df_clean.corr()

# Extraire les corrélations en excluant la diagonale
corr_pairs = corr_matrix.unstack()
corr_pairs = corr_pairs[corr_pairs != 1.0]  # Exclure auto-corrélations
corr_pairs = corr_pairs.drop_duplicates()
corr_pairs = corr_pairs.sort_values(ascending=False)

print("\nTop 10 Corrélations Positives:")
print(corr_pairs.head(10))

print("\nTop 10 Corrélations Négatives:")
print(corr_pairs.tail(10))

print("\n")

# ===============================================
# 11. RÉSUMÉ FINAL
# ===============================================
print("="*70)
print("RÉSUMÉ DE L'ANALYSE")
print("="*70)
print(f"✓ Dataset: Air Quality UCI")
print(f"✓ Période: Mars 2004 - Février 2005")
print(f"✓ Nombre total d'observations: {len(df)}")
print(f"✓ Nombre de variables: {len(df.columns)}")
print(f"✓ Variables numériques: {len(numeric_cols)}")
print(f"✓ Taille du dataset: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print("\n✓ Analyse terminée avec succès!")
print("="*70)